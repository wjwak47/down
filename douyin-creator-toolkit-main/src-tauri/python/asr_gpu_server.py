"""
GPU åŠ é€Ÿè¯­éŸ³è¯†åˆ«æœåŠ¡
ä½¿ç”¨ sherpa-onnx Python ç»‘å®š + SenseVoice æ¨¡å‹
æ”¯æŒ DirectML GPU åŠ é€Ÿ (Windows)
"""

import os
import sys
import time
import wave
import struct
from pathlib import Path
from typing import Optional
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="ASR GPU Server")

# å…¨å±€æ¨¡å‹å®ä¾‹
recognizer = None
current_device = "cpu"
current_num_threads = 4
model_loaded = False

class TranscribeResponse(BaseModel):
    status: str
    text: Optional[str] = None
    duration_ms: Optional[int] = None
    device: Optional[str] = None
    rtf: Optional[float] = None
    error: Optional[str] = None

class StatusResponse(BaseModel):
    status: str
    model_loaded: bool
    device: str
    gpu_available: bool
    gpu_name: Optional[str] = None

class LoadRequest(BaseModel):
    use_gpu: bool = True
    gpu_device_id: int = 0

def get_models_dir() -> Path:
    """è·å–æ¨¡å‹ç›®å½•"""
    if "ASR_MODELS_DIR" in os.environ:
        return Path(os.environ["ASR_MODELS_DIR"])
    
    if sys.platform == "win32":
        # ä½¿ç”¨ Roaming AppData ç›®å½•
        app_data = Path(os.environ.get("APPDATA", ""))
        return app_data / "DouyinCreatorToolkit" / "models" / "asr" / "sense-voice"
    else:
        return Path.home() / ".local" / "share" / "DouyinCreatorToolkit" / "models" / "asr" / "sense-voice"

def detect_gpu() -> tuple:
    """æ£€æµ‹å¯ç”¨çš„ GPUï¼Œä¼˜å…ˆè¿”å› NVIDIA/AMD ç‹¬ç«‹æ˜¾å¡"""
    try:
        if sys.platform == "win32":
            import subprocess
            result = subprocess.run(
                ["wmic", "path", "win32_VideoController", "get", "name"],
                capture_output=True, text=True
            )
            lines = [l.strip() for l in result.stdout.split('\n') if l.strip() and l.strip() != "Name"]
            
            if lines:
                # ä¼˜å…ˆé€‰æ‹© NVIDIA æˆ– AMD ç‹¬ç«‹æ˜¾å¡
                for i, gpu in enumerate(lines):
                    if "NVIDIA" in gpu.upper() or "AMD" in gpu.upper() or "RADEON" in gpu.upper():
                        print(f"[ASR-GPU] æ£€æµ‹åˆ°ç‹¬ç«‹æ˜¾å¡: {gpu} (ç´¢å¼•: {i})")
                        return True, gpu, i
                # å¦‚æœæ²¡æœ‰ç‹¬ç«‹æ˜¾å¡ï¼Œè¿”å›ç¬¬ä¸€ä¸ª
                return True, lines[0], 0
    except Exception as e:
        print(f"[ASR-GPU] GPU æ£€æµ‹å¤±è´¥: {e}")
    
    return False, None, 0

def read_wav_samples(audio_path: str) -> tuple:
    """è¯»å– WAV æ–‡ä»¶å¹¶è¿”å›é‡‡æ ·æ•°æ®"""
    with wave.open(audio_path, 'rb') as wav:
        sample_rate = wav.getframerate()
        n_frames = wav.getnframes()
        n_channels = wav.getnchannels()
        sample_width = wav.getsampwidth()
        
        raw_data = wav.readframes(n_frames)
        
        # è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        if sample_width == 2:
            fmt = f"<{n_frames * n_channels}h"
            samples = list(struct.unpack(fmt, raw_data))
            samples = [s / 32768.0 for s in samples]
        elif sample_width == 4:
            fmt = f"<{n_frames * n_channels}i"
            samples = list(struct.unpack(fmt, raw_data))
            samples = [s / 2147483648.0 for s in samples]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é‡‡æ ·ä½æ·±: {sample_width * 8}")
        
        # è½¬æ¢ä¸ºå•å£°é“
        if n_channels == 2:
            samples = [(samples[i] + samples[i+1]) / 2 for i in range(0, len(samples), 2)]
        
        duration_ms = int(len(samples) / sample_rate * 1000)
        return samples, sample_rate, duration_ms

def get_best_directml_device_id(models_dir: Path) -> int:
    """
    æ™ºèƒ½æ¢æµ‹æœ€ä½³ DirectML è®¾å¤‡ IDã€‚
    ç­–ç•¥ï¼šä¼˜å…ˆå°è¯• ID 1 (ç¬”è®°æœ¬ç‹¬æ˜¾)ï¼Œå¤±è´¥åˆ™å›é€€ ID 0 (å°å¼æœº/é›†æ˜¾)ï¼Œå‡å¤±è´¥åˆ™å›é€€ CPU (-1)ã€‚
    """
    import sherpa_onnx
    
    # æ„é€ ä¸€ä¸ªæœ€å°åŒ–çš„æ¢æµ‹é…ç½®
    model_path = str(models_dir / "model.onnx")
    tokens_path = str(models_dir / "tokens.txt")
    
    # å¦‚æœæ²¡æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œæ— æ³•æ¢æµ‹
    if not (models_dir / "model.onnx").exists():
        print(f"[ASR-GPU] æ¢æµ‹å¤±è´¥: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ {model_path}")
        return 0 # é»˜è®¤å›é€€åˆ° 0ï¼Œåç»­æµç¨‹ä¼šæŠ¥é”™

    def probe_device(device_id):
        """å‘é€ä¸€ä¸ª'æ¢é’ˆ'ï¼Œçœ‹è®¾å¤‡æ˜¯å¦èƒ½å“åº”"""
        try:
            print(f"[ASR-GPU] ğŸ” æ­£åœ¨æ¢æµ‹ DirectML è®¾å¤‡ ID: {device_id}...")
            
            # ä½¿ç”¨æœ€ç®€é…ç½®ï¼Œdebug=False å‡å°‘æ—¥å¿—
            provider = f"dml:{device_id}"
            
            # ä½¿ç”¨ from_sense_voice å·¥å‚æ–¹æ³•
            _ = sherpa_onnx.OfflineRecognizer.from_sense_voice(
                model=model_path,
                tokens=tokens_path,
                num_threads=1,
                provider=provider,
                language="auto",
                use_itn=False,
                debug=False,
            )
            
            print(f"[ASR-GPU] âœ… è®¾å¤‡ ID {device_id} åˆå§‹åŒ–æˆåŠŸï¼")
            return True
        except Exception as e:
            # ç®€åŒ–é”™è¯¯æ—¥å¿—ï¼Œå–ç¬¬ä¸€è¡Œ
            err_msg = str(e).split('\n')[0]
            print(f"[ASR-GPU] âŒ è®¾å¤‡ ID {device_id} ä¸å¯ç”¨ã€‚åŸå› : {err_msg}...")
            return False

    # === æ ¸å¿ƒç­–ç•¥ ===
    
    # 1. ä¼˜å…ˆæ¢æµ‹ ID 1
    # ç†ç”±ï¼šåœ¨ Windows åŒæ˜¾å¡ç¬”è®°æœ¬ä¸Šï¼ŒID 1 å‡ ä¹ 100% æ˜¯ç‹¬ç«‹æ˜¾å¡ï¼ˆNVIDIA/AMDï¼‰ã€‚
    if probe_device(1):
        print("[ASR-GPU] ğŸš€ é€‰ä¸­ç­–ç•¥ï¼šé«˜æ€§èƒ½ç‹¬ç«‹æ˜¾å¡ (ID: 1)")
        return 1
        
    # 2. å›é€€æ¢æµ‹ ID 0
    # ç†ç”±ï¼šå¦‚æœ ID 1 ä¸å­˜åœ¨ï¼Œè¯´æ˜ç”¨æˆ·æ˜¯å•æ˜¾å¡ç¯å¢ƒï¼ˆå°å¼æœºç‹¬æ˜¾ æˆ– ç¬”è®°æœ¬çº¯é›†æ˜¾ï¼‰ã€‚
    if probe_device(0):
        print("[ASR-GPU] ğŸš— é€‰ä¸­ç­–ç•¥ï¼šé»˜è®¤æ˜¾ç¤ºé€‚é…å™¨ (ID: 0)")
        return 0
        
    # 3. å½»åº•å¤±è´¥
    print("[ASR-GPU] âš ï¸ æœªæ£€æµ‹åˆ°æ”¯æŒ DirectML çš„ GPUï¼Œå°†å›é€€åˆ° CPUã€‚")
    return -1

def load_model(use_gpu: bool = True, gpu_device_id: int = 0, num_threads: int = 4) -> bool:
    """åŠ è½½ SenseVoice æ¨¡å‹"""
    global recognizer, current_device, model_loaded, current_num_threads
    
    current_num_threads = num_threads
    
    try:
        import sherpa_onnx
        
        models_dir = get_models_dir()
        print(f"[ASR-GPU] æ¨¡å‹ç›®å½•: {models_dir}")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        model_file = models_dir / "model.onnx"
        model_int8 = models_dir / "model.int8.onnx"
        tokens_file = models_dir / "tokens.txt"
        
        # ä¼˜å…ˆä½¿ç”¨éé‡åŒ–æ¨¡å‹ (GPU å‹å¥½)
        if model_file.exists():
            actual_model = str(model_file)
        elif model_int8.exists():
            actual_model = str(model_int8)
        else:
            print(f"[ASR-GPU] æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file}")
            return False
        
        if not tokens_file.exists():
            print(f"[ASR-GPU] tokens æ–‡ä»¶ä¸å­˜åœ¨: {tokens_file}")
            return False
        
        # æ™ºèƒ½é€‰æ‹© providerï¼šå°è¯• CUDAï¼Œå¤±è´¥å›é€€ CPU
        provider = "cpu"
        current_device = "CPU"
        # use passed num_threads
        
        if use_gpu:
            # æ£€æŸ¥æ˜¯å¦æœ‰ NVIDIA GPU
            gpu_available, gpu_name, _ = detect_gpu()
            if gpu_available and "nvidia" in gpu_name.lower():
                print(f"[ASR-GPU] æ£€æµ‹åˆ° NVIDIA GPU: {gpu_name}ï¼Œå°è¯• CUDA åŠ é€Ÿ...")
                try:
                    # å…ˆå°è¯•ç”¨ CUDA åŠ è½½
                    test_recognizer = sherpa_onnx.OfflineRecognizer.from_sense_voice(
                        model=actual_model,
                        tokens=str(tokens_file),
                        num_threads=4,
                        provider="cuda",
                        language="auto",
                        use_itn=True,
                        debug=False,
                    )
                    # æˆåŠŸäº†ï¼
                    provider = "cuda"
                    current_device = "GPU (CUDA)"
                    # num_threads for CUDA is typically handled internally or set to a low value
                    # but we keep user value or force to smaller if needed?
                    # for now, keep as is or let user decide. Actually sherpa might ignore it for CUDA.
                    print("[ASR-GPU] âœ… CUDA åŠ é€Ÿå·²å¯ç”¨ï¼")
                    # ç›´æ¥ä½¿ç”¨è¿™ä¸ª recognizer
                    recognizer = test_recognizer
                except Exception as e:
                    print(f"[ASR-GPU] âš ï¸ CUDA åˆå§‹åŒ–å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    print("[ASR-GPU] å›é€€åˆ° CPU æ¨¡å¼")
            else:
                print(f"[ASR-GPU] æœªæ£€æµ‹åˆ° NVIDIA GPU (å½“å‰: {gpu_name})ï¼Œä½¿ç”¨ CPU æ¨¡å¼")
        
        print(f"[ASR-GPU] æœ€ç»ˆé…ç½®: Provider={provider}, Threads={num_threads}")
        
        start_time = time.time()
        
        # å¦‚æœè¿˜æ²¡åˆ›å»º recognizerï¼ˆæ²¡èµ° CUDA åˆ†æ”¯æˆ– CUDA å¤±è´¥ï¼‰
        if provider == "cpu":
            recognizer = sherpa_onnx.OfflineRecognizer.from_sense_voice(
                model=actual_model,
                tokens=str(tokens_file),
                num_threads=num_threads,
                provider=provider,
                language="auto",
                use_itn=True,
                debug=True,
            )
        
        load_time = time.time() - start_time
        print(f"[ASR-GPU] æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}s")
        print(f"[ASR-GPU] å½“å‰è®¾å¤‡: {current_device}")
        
        model_loaded = True
        return True
        
    except Exception as e:
        print(f"[ASR-GPU] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        current_device = "Error"
        return False

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "service": "asr-gpu"}

@app.get("/status")
async def get_status() -> StatusResponse:
    """è·å–æœåŠ¡çŠ¶æ€"""
    gpu_available, gpu_name, _ = detect_gpu()
    return StatusResponse(
        status="ok",
        model_loaded=model_loaded,
        device=current_device,
        gpu_available=gpu_available,
        gpu_name=gpu_name
    )

@app.post("/load")
async def load_model_endpoint(request: LoadRequest = None):
    """åŠ è½½æ¨¡å‹"""
    use_gpu = request.use_gpu if request else True
    gpu_device_id = request.gpu_device_id if request else 0
    
    success = load_model(use_gpu=use_gpu, gpu_device_id=gpu_device_id)
    if success:
        return {"status": "ok", "message": "æ¨¡å‹åŠ è½½æˆåŠŸ", "device": current_device}
    else:
        raise HTTPException(status_code=500, detail="æ¨¡å‹åŠ è½½å¤±è´¥")

@app.post("/unload")
async def unload_model():
    """å¸è½½æ¨¡å‹"""
    global recognizer, model_loaded
    recognizer = None
    model_loaded = False
    return {"status": "ok", "message": "æ¨¡å‹å·²å¸è½½"}

@app.post("/transcribe")
async def transcribe(request: TranscribeRequest) -> TranscribeResponse:
    """è½¬å†™éŸ³é¢‘æ–‡ä»¶"""
    global recognizer, current_device, model_loaded
    
    audio_path = request.audio_path
    if not os.path.exists(audio_path):
        return TranscribeResponse(
            status="error",
            error=f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}"
        )
    
    
    # å¦‚æœè¯·æ±‚çš„æ¨¡å¼å’Œå½“å‰æ¨¡å¼ä¸åŒï¼Œæˆ–è€…çº¿ç¨‹æ•°å˜äº†ï¼Œé‡æ–°åŠ è½½æ¨¡å‹
    requested_gpu = request.use_gpu
    requested_threads = request.num_threads or 4
    current_is_gpu = "GPU" in current_device
    
    if recognizer is None or requested_gpu != current_is_gpu or (requested_gpu is False and requested_threads != current_num_threads):
        print(f"[ASR-GPU] é…ç½®å˜æ›´: GPU={requested_gpu}, Threads={requested_threads}")
        if not load_model(use_gpu=requested_gpu, num_threads=requested_threads):
            return TranscribeResponse(
                status="error",
                error="æ¨¡å‹åŠ è½½å¤±è´¥"
            )
    
    try:
        print(f"[ASR-GPU] å¼€å§‹è½¬å†™: {audio_path}")
        start_time = time.time()
        
        # è¯»å–éŸ³é¢‘
        samples, sample_rate, duration_ms = read_wav_samples(audio_path)
        
        # åˆ›å»ºæµå¹¶è½¬å†™
        stream = recognizer.create_stream()
        stream.accept_waveform(sample_rate, samples)
        recognizer.decode_stream(stream)
        
        text = stream.result.text
        
        transcribe_time = time.time() - start_time
        rtf = transcribe_time / (duration_ms / 1000) if duration_ms > 0 else 0
        
        print(f"[ASR-GPU] è½¬å†™å®Œæˆ:")
        print(f"  éŸ³é¢‘æ—¶é•¿: {duration_ms/1000:.2f}s")
        print(f"  è½¬å†™è€—æ—¶: {transcribe_time:.2f}s")
        print(f"  å®æ—¶ç‡ (RTF): {rtf:.3f}x")
        print(f"  è®¾å¤‡: {current_device}")
        
        return TranscribeResponse(
            status="success",
            text=text.strip(),
            duration_ms=duration_ms,
            device=current_device,
            rtf=rtf
        )
        
    except Exception as e:
        print(f"[ASR-GPU] è½¬å†™å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return TranscribeResponse(
            status="error",
            error=str(e)
        )

# é…ç½®
ASR_PORT = int(os.environ.get("ASR_GPU_PORT", "38081"))
ASR_HOST = os.environ.get("ASR_GPU_HOST", "127.0.0.1")

if __name__ == "__main__":
    print("[ASR-GPU] å¯åŠ¨ GPU è¯­éŸ³è¯†åˆ«æœåŠ¡...")
    print(f"[ASR-GPU] æœåŠ¡åœ°å€: http://{ASR_HOST}:{ASR_PORT}")
    
    # å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹ (GPU æ¨¡å¼)
    load_model(use_gpu=True, gpu_device_id=0)
    
    uvicorn.run(app, host=ASR_HOST, port=ASR_PORT)
